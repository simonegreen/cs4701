{"cells":[{"cell_type":"markdown","metadata":{},"source":["# News Headline Generation"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1: Data Preparation"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T03:37:23.221995Z","iopub.status.busy":"2023-12-09T03:37:23.221638Z","iopub.status.idle":"2023-12-09T03:37:36.627189Z","shell.execute_reply":"2023-12-09T03:37:36.626244Z","shell.execute_reply.started":"2023-12-09T03:37:23.221966Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n","  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"]}],"source":["import numpy as np\n","import matplotlib.pylab as plt\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","import string\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten, LSTM, Embedding\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras.optimizers import RMSprop\n","from keras.applications.densenet import preprocess_input,decode_predictions\n","from keras.preprocessing.text import Tokenizer\n","from keras.utils import pad_sequences\n","import keras.utils as ku\n","#from keras.preprocessing.sequence import pad_sequencese\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T03:37:36.630275Z","iopub.status.busy":"2023-12-09T03:37:36.629253Z","iopub.status.idle":"2023-12-09T03:37:37.587897Z","shell.execute_reply":"2023-12-09T03:37:37.586889Z","shell.execute_reply.started":"2023-12-09T03:37:36.630229Z"},"trusted":true},"outputs":[],"source":["df2 = pd.read_csv(\"/kaggle/input/selected-sources/selected_sources.csv\")\n","#sources_w_art = df2[['year', 'title', 'article', 'publication']]\n","sources = df2[['title', 'publication']]"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T03:37:40.278544Z","iopub.status.busy":"2023-12-09T03:37:40.277733Z","iopub.status.idle":"2023-12-09T03:37:46.100768Z","shell.execute_reply":"2023-12-09T03:37:46.099534Z","shell.execute_reply.started":"2023-12-09T03:37:40.278509Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(413999, 2)\n","<bound method NDFrame.head of                                                     title       publication\n","0       we should take concerns about the health of li...               Vox\n","1       colts gm ryan grigson says andrew lucks contra...  Business Insider\n","2       paris hilton woman in black for uncle montys f...               TMZ\n","3                 how to watch the google io keynote live               Vox\n","4       “elizabeth warren called me” is turning into a...               Vox\n","...                                                   ...               ...\n","413994  florida ammo selling out on heels of stayathom...               TMZ\n","413995  disney forcing annual pass holders to continue...               TMZ\n","413996  nick cannon pimps out his impala with custom n...               TMZ\n","413997  pete buttigieg says governors showing more lea...               TMZ\n","413998  ruth bader ginsburg still working out with tra...               TMZ\n","\n","[413989 rows x 2 columns]>\n","(413989, 2)\n"]}],"source":["'''\n","Should we remove punctuation? There might be some pros and cons, but references seem to remove it.\n","\n","Here we are cleaning our data\n","'''\n","\n","\n","sources.head()\n","print(sources.shape)\n","sources = sources[sources['title'].apply(lambda x: isinstance(x, str))]\n","sources = sources[sources['title'].apply(lambda x: len(x.split()) <= 30)]\n","sources['title'] = sources['title'].apply(lambda x: x.lower())\n","sources['title'] = sources['title'].apply(lambda x: x.strip())\n","\n","def removePunc(str):\n","  str = \"\".join(i for i in str if i not in string.punctuation)\n","  return str\n","\n","sources[\"title\"] = sources['title'].apply(lambda x: removePunc(x))\n","print(sources.head)\n","\n","print(sources.shape)"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T03:37:49.354758Z","iopub.status.busy":"2023-12-09T03:37:49.354351Z","iopub.status.idle":"2023-12-09T03:37:49.794238Z","shell.execute_reply":"2023-12-09T03:37:49.793237Z","shell.execute_reply.started":"2023-12-09T03:37:49.354715Z"},"trusted":true},"outputs":[],"source":["foxSources = sources.loc[sources['publication'] == \"Fox News\"]\n","foxSources.reset_index(inplace=True)\n","voxSources = sources.loc[sources['publication'] == \"Vox\"]\n","cnnSources = sources.loc[sources['publication'] == \"CNN\"]\n","tmzSources = sources.loc[sources['publication'] == \"TMZ\"]\n","refinerySources = sources.loc[sources['publication'] == \"Refinery 29\"]\n","bizSources = sources.loc[sources['publication'] == \"Business Insider\"]"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T03:37:51.894395Z","iopub.status.busy":"2023-12-09T03:37:51.893989Z","iopub.status.idle":"2023-12-09T03:37:51.900521Z","shell.execute_reply":"2023-12-09T03:37:51.899266Z","shell.execute_reply.started":"2023-12-09T03:37:51.894366Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(20144, 3)\n","(47272, 2)\n","(127594, 2)\n","(49595, 2)\n","(111432, 2)\n","(57952, 2)\n"]}],"source":["print(foxSources.shape)\n","print(voxSources.shape)\n","print(cnnSources.shape)\n","print(tmzSources.shape)\n","print(refinerySources.shape)\n","print(bizSources.shape)"]},{"cell_type":"markdown","metadata":{},"source":["### Tokenization & Flattening"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:04:27.989476Z","iopub.status.busy":"2023-12-09T04:04:27.989044Z","iopub.status.idle":"2023-12-09T04:04:27.998613Z","shell.execute_reply":"2023-12-09T04:04:27.997519Z","shell.execute_reply.started":"2023-12-09T04:04:27.989443Z"},"trusted":true},"outputs":[],"source":["vocabSize = 10000\n","tokenizer = Tokenizer(num_words=vocabSize, oov_token = \"<OOV>\")\n","\n","def textToToken(df):\n","  # Updating tokenizer vocabulary to only contains words in df\n","  tokenizer.fit_on_texts(df[\"title\"])\n","  # Getting the size of the vocabulary\n","  # vocabSize = len(tokenizer.word_index) + 1\n","  inputs = []\n","  for title in df['title']:\n","    # Converts all text into tokens in array form like [8, 9, 2, 10, 11, 3, 1]\n","    tokens = tokenizer.texts_to_sequences([title])[0]\n","    for x in range(1, len(tokens)):\n","      seq = tokens[:x+1]\n","      inputs.append(seq)\n","  return inputs #, vocabSize\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Padding"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T03:38:01.624362Z","iopub.status.busy":"2023-12-09T03:38:01.623970Z","iopub.status.idle":"2023-12-09T03:38:01.632688Z","shell.execute_reply":"2023-12-09T03:38:01.630893Z","shell.execute_reply.started":"2023-12-09T03:38:01.624332Z"},"trusted":true},"outputs":[],"source":["#padding sequences\n","#get input from output of tokenizer\n","\n","def generate_padded_sequences(input_sequences, total_words):\n","    max_sequence_length = max([len(x) for x in input_sequences])\n","    input_sequences = np.array(pad_sequences(input_sequences, maxlen = max_sequence_length, padding = 'pre'))\n","\n","    predictors = input_sequences[:,:-1]\n","    label = input_sequences[:,-1]\n","\n","    label = ku.to_categorical(label, num_classes = total_words)\n","    return predictors, label, max_sequence_length"]},{"cell_type":"markdown","metadata":{},"source":["### Ensuring Meaningful Words"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we want to ignore any words that are in the context of OOV words because this will mess up the process of contextualizing surrounding words.\n","# this means that if a word appears AFTER an OOV word, we don't want to include their token as a possible option. in the event that this word\n","# appears in another context preceding an OOV word, its token will be added.\n","# we also want to ensure that we have a decent semantic construction around the word, so we should instill a minimum size that we will parse\n","\n","def semanticCleaning()"]},{"cell_type":"markdown","metadata":{},"source":["## Part 2: Source Specific Models"]},{"cell_type":"markdown","metadata":{},"source":["### Fox"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:04:38.864465Z","iopub.status.busy":"2023-12-09T04:04:38.864023Z","iopub.status.idle":"2023-12-09T04:04:41.507026Z","shell.execute_reply":"2023-12-09T04:04:41.505999Z","shell.execute_reply.started":"2023-12-09T04:04:38.864431Z"},"trusted":true},"outputs":[],"source":["inputs = textToToken(foxSources)\n","predictors, label, max_sequence_length = generate_padded_sequences(inputs, vocabSize)\n"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:00:39.967897Z","iopub.status.busy":"2023-12-09T04:00:39.967208Z","iopub.status.idle":"2023-12-09T04:00:39.978368Z","shell.execute_reply":"2023-12-09T04:00:39.977299Z","shell.execute_reply.started":"2023-12-09T04:00:39.967867Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["baseball capsules\n","washington nationals at miami marlins game preview\n","metsbraves preview\n","cubsphillies preview\n","217 is preview\n"]},{"data":{"text/plain":["[[1492, 1],\n"," [335, 3466],\n"," [335, 3466, 11],\n"," [335, 3466, 11, 1377],\n"," [335, 3466, 11, 1377, 1],\n"," [335, 3466, 11, 1377, 1, 177],\n"," [335, 3466, 11, 1377, 1, 177, 217],\n"," [1, 217],\n"," [1, 217],\n"," [1, 217]]"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["print(foxSources['title'][0])\n","print(foxSources['title'][1])\n","print(foxSources['title'][2])\n","print(foxSources['title'][3])\n","\n","print(inputs[6][-1], \"is preview\")\n","\n","\n","inputs[:10]"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:04:45.325691Z","iopub.status.busy":"2023-12-09T04:04:45.325278Z","iopub.status.idle":"2023-12-09T04:04:45.333484Z","shell.execute_reply":"2023-12-09T04:04:45.332380Z","shell.execute_reply.started":"2023-12-09T04:04:45.325658Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["5000\n","5000\n","\n","162682\n","162682\n","\n","(array([3465]),)\n"]}],"source":["print(vocabSize)\n","print(len(label[0]))\n","print(\"\")\n","print(len(label))\n","print(len(inputs))\n","print(\"\")\n","x = np.where(label[0] == 1)\n","print(x)\n","\n","# this is checking that label works correctly. label is an array, \n","# with n rows (n = # of inputs / len of inputs) and vocabSize columns.\n","# for each input, there is a 1 at the index corresponding to the ending index on\n","# the input (eg for input 0, there is a 1 at 9288) and a 0 at all other indices.\n","# the label denotes what the ending word is (bc the index is where the word is\n","# located)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:04:49.103045Z","iopub.status.busy":"2023-12-09T04:04:49.102633Z","iopub.status.idle":"2023-12-09T04:04:49.453314Z","shell.execute_reply":"2023-12-09T04:04:49.452251Z","shell.execute_reply.started":"2023-12-09T04:04:49.103012Z"},"trusted":true},"outputs":[],"source":["foxModel = Sequential()\n","input_len = max_sequence_length - 1\n","foxModel.add(Embedding(vocabSize, 10, input_length = input_len))\n","foxModel.add(Dropout(0.4)) # dropout to avoid overfitting\n","foxModel.add(LSTM(100))    \n","foxModel.add(Dropout(0.4))\n","foxModel.add(Dense(vocabSize, activation='softmax'))\n","\n","\n","foxModel.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n","  \n","  "]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:04:52.771998Z","iopub.status.busy":"2023-12-09T04:04:52.771573Z","iopub.status.idle":"2023-12-09T04:04:52.799422Z","shell.execute_reply":"2023-12-09T04:04:52.798321Z","shell.execute_reply.started":"2023-12-09T04:04:52.771965Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 20, 10)            50000     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 20, 10)            0         \n","                                                                 \n"," lstm_1 (LSTM)               (None, 100)               44400     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 100)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5000)              505000    \n","                                                                 \n","=================================================================\n","Total params: 599400 (2.29 MB)\n","Trainable params: 599400 (2.29 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["foxModel.summary()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:11:07.493613Z","iopub.status.busy":"2023-12-09T04:11:07.493206Z","iopub.status.idle":"2023-12-09T04:26:14.147597Z","shell.execute_reply":"2023-12-09T04:26:14.146654Z","shell.execute_reply.started":"2023-12-09T04:11:07.493568Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.3932\n","Epoch 2/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.3621\n","Epoch 3/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.3322\n","Epoch 4/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.3105\n","Epoch 5/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.2803\n","Epoch 6/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.2637\n","Epoch 7/30\n","5084/5084 [==============================] - 31s 6ms/step - loss: 6.2466\n","Epoch 8/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.2237\n","Epoch 9/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.2085\n","Epoch 10/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.1942\n","Epoch 11/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.1796\n","Epoch 12/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.1695\n","Epoch 13/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.1575\n","Epoch 14/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.1398\n","Epoch 15/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.1359\n","Epoch 16/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.1275\n","Epoch 17/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.1153\n","Epoch 18/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.1111\n","Epoch 19/30\n","5084/5084 [==============================] - 29s 6ms/step - loss: 6.1017\n","Epoch 20/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.0973\n","Epoch 21/30\n","5084/5084 [==============================] - 31s 6ms/step - loss: 6.0899\n","Epoch 22/30\n","5084/5084 [==============================] - 31s 6ms/step - loss: 6.0831\n","Epoch 23/30\n","5084/5084 [==============================] - 31s 6ms/step - loss: 6.0784\n","Epoch 24/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.0724\n","Epoch 25/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.0666\n","Epoch 26/30\n","5084/5084 [==============================] - 31s 6ms/step - loss: 6.0594\n","Epoch 27/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.0569\n","Epoch 28/30\n","5084/5084 [==============================] - 31s 6ms/step - loss: 6.0546\n","Epoch 29/30\n","5084/5084 [==============================] - 31s 6ms/step - loss: 6.0549\n","Epoch 30/30\n","5084/5084 [==============================] - 30s 6ms/step - loss: 6.0423\n"]}],"source":["foxHistory = foxModel.fit(predictors, label, epochs=30)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:29:06.738419Z","iopub.status.busy":"2023-12-09T04:29:06.738001Z","iopub.status.idle":"2023-12-09T04:29:06.745772Z","shell.execute_reply":"2023-12-09T04:29:06.744613Z","shell.execute_reply.started":"2023-12-09T04:29:06.738387Z"},"trusted":true},"outputs":[],"source":["def generate_text(seed_text, next_words, model, max_sequence_len):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","        predicted = model.predict(token_list)\n","        idx = np.argmax(predicted)\n","        next = tokenizer.sequences_to_texts([[idx]])\n","#         for word,index in tokenizer.word_index.items():\n","#             if index == idx:\n","#                 output_word = word\n","#                 break\n","        seed_text += \" \"+next[0]\n","    return seed_text.title()"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-12-09T04:29:09.374263Z","iopub.status.busy":"2023-12-09T04:29:09.373322Z","iopub.status.idle":"2023-12-09T04:29:10.485338Z","shell.execute_reply":"2023-12-09T04:29:10.484148Z","shell.execute_reply.started":"2023-12-09T04:29:09.374225Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 441ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 23ms/step\n","1/1 [==============================] - 0s 23ms/step\n","United States To Be In The Us\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 22ms/step\n","1/1 [==============================] - 0s 24ms/step\n","1/1 [==============================] - 0s 23ms/step\n","Donald Trump Is A A The Of\n"]}],"source":["print(generate_text(\"united states\", 5, foxModel, max_sequence_length))\n","print(generate_text(\"donald trump\", 5, foxModel, max_sequence_length))\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### CNN"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#Model\n","\n","def create_model(max_sequence_length, total_words):\n","    input_len = max_sequence_length -1\n","\n","    model = Sequential()\n","\n","\n","    #Embedding Layer\n","    model.add(Embedding(total_words, 10, input_length = input_len))\n","\n","    model.add(Dropout(0.1))\n","\n","    #Hidden Layer 1\n","    model.add(LSTM(100))\n","    model.add(Dropout(0.1))\n","\n","\n","    model.add(Dense(total_words, activation = 'softmax'))\n","\n","    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n","\n","    return model\n","\n","model = create_model(max_sequence_length, vocabSize)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["### Vox"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["### Model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_word_count = 0\n","title_with_most_words = \"\"\n","for title in sources['title']:\n","    if type(title) == float:\n","      print(title)\n","    else:\n","      words = title.split()\n","    \n","    # Get the word count for the current title\n","    word_count = len(words)\n","    \n","    # Check if the current title has more words than the previous maximum\n","    if word_count > max_word_count:\n","        max_word_count = word_count\n","        title_with_most_words = title\n","print(title_with_most_words, max_word_count)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["classes = np.unique(sources['publication'], return_counts=True)\n","classes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Split the data into training and test sets, stratified by the 'publication' category\n","X_train, X_test, y_train, y_test = train_test_split(\n","    sources['title'],  # Features\n","    sources['publication'],  # Target variable\n","    test_size=0.1,  # 10% for the test set\n","    stratify=sources['publication'],  # Stratify by 'publication' category\n","    random_state=42  # Set a random seed for reproducibility\n",")\n","print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_num_words = 10000\n","seq_len = 40\n","embedding_size = 100\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","tokenizer = Tokenizer(num_words=max_num_words) #Tokenizer is used to tokenize text\n","tokenizer.fit_on_texts(X_train) #Fit this to our corpus\n","\n","x_train = tokenizer.texts_to_sequences(X_train) #'text to sequences converts the text to a list of indices\n","x_train = pad_sequences(x_train, maxlen=40) #pad_sequences makes every sequence a fixed size list by padding with 0s \n","\n","\n","x_test = tokenizer.texts_to_sequences(X_test) \n","x_test = pad_sequences(x_test, maxlen=40)\n","\n","x_train.shape, x_test.shape # Check the dimensions of x_train and x_test "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x_train[2]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["unique_labels = list(y_train.unique())\n","print(unique_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_classes = len(np.unique(y_train))\n","\n","# Tokenize and pad your sequences as you have done before\n","\n","# Create a label encoder to encode the labels\n","label_encoder = LabelEncoder()\n","y_train_encoded = label_encoder.fit_transform(y_train)\n","y_test_encoded = label_encoder.transform(y_test)\n","\n","# Create the LSTM model\n","model = Sequential()\n","\n","# Add an embedding layer to convert words to dense vectors\n","model.add(Embedding(input_dim=max_num_words, output_dim=embedding_size, input_length=seq_len))\n","\n","# Add an LSTM layer\n","model.add(LSTM(64, return_sequences=False))\n","\n","# Add a dense layer with softmax activation for classification\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","# Compile the model\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(x_train, y_train_encoded, epochs=10, batch_size=32, validation_data=(x_test, y_test_encoded))\n","\n","# Evaluate the model\n","loss, accuracy = model.evaluate(x_test, y_test_encoded)\n","print(f'Loss: {loss}, Accuracy: {accuracy}')"]},{"cell_type":"markdown","metadata":{},"source":["### selected news sources\n","all at least 20,000 publications\n","\n","<b>Everything</b>\n","- Fox News (right - 20,144)\n","- Vox (left - )\n","- CNN (left center - 127,602)\n","\n","<b>Entertainment News</b>\n","- TMZ (49,595)\n","- Refinery29 (111,433)\n","\n","<b>Business</b>\n","- Business Insider (57,953)"]},{"cell_type":"markdown","metadata":{},"source":["### Don't need to run again period, just for initial set up"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df2 = pd.read_csv(\"all-the-news-2-1.csv\")\n","#sources_w_art = df2[['year', 'title', 'article', 'publication']]\n","sources = df2[['title', 'publication']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selected = ['Fox News', 'Vox', 'CNN', 'TMZ', 'Refinery 29', 'Business Insider']\n","sources = sources.loc[sources['publication'].isin(selected)]\n","sources = sources[['title', 'publication']]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sources.to_csv('selected_sources.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sources"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4129712,"sourceId":7152252,"sourceType":"datasetVersion"}],"dockerImageVersionId":30616,"isGpuEnabled":true,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
